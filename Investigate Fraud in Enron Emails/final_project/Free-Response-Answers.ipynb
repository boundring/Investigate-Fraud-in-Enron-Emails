{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Identify Fraud from Enron Email\n",
    "### Enron Submission Free-Response Answers\n",
    "\n",
    "Student: Richard Smith\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 1\n",
    "> Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?\n",
    "\n",
    "### Project Goal\n",
    "\n",
    "Regarding the goal of the project, this excerpt from the Project Overview is the most direct answer:\n",
    "> In this project, you will play detective, and put your machine learning skills to use by building an algorithm to identify Enron Employees who may have committed fraud based on the public Enron financial and email dataset.\n",
    "\n",
    "In other words, the goal of the project is to train a machine learning algorithm to correctly predict the value of 'poi' for an individual, given financial data and email metadata derived from the public database of Enron emails.\n",
    "\n",
    "### Dataset Background\n",
    "\n",
    "Regarding the public Enron financial and email dataset, this excerpt from the Project Overview is an appropriate description:\n",
    "> In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. In this project, you will play detective, and put your new skills to use by building a person of interest identifier based on financial and email data made public as a result of the Enron scandal. To assist you in your detective work, we've combined this data with a hand-generated list of persons of interest in the fraud case, which means individuals who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity.\n",
    "\n",
    "The specific data students are given is \"final_project_dataset.pkl\", which contains rows of financial data and email metadata for various people (and one business) involved with the Enron scandal, either by way of their working for Enron, or else being associated with those working for Enron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows present in the data: 146\n",
      "Data features for any given row:\n",
      "['salary',\n",
      " 'to_messages',\n",
      " 'deferral_payments',\n",
      " 'total_payments',\n",
      " 'loan_advances',\n",
      " 'bonus',\n",
      " 'email_address',\n",
      " 'restricted_stock_deferred',\n",
      " 'deferred_income',\n",
      " 'total_stock_value',\n",
      " 'expenses',\n",
      " 'from_poi_to_this_person',\n",
      " 'exercised_stock_options',\n",
      " 'from_messages',\n",
      " 'other',\n",
      " 'from_this_person_to_poi',\n",
      " 'poi',\n",
      " 'long_term_incentive',\n",
      " 'shared_receipt_with_poi',\n",
      " 'restricted_stock',\n",
      " 'director_fees']\n",
      "Number of features: 21\n",
      "Number of persons of interest in the set:  18\n",
      "Non-PoIs:  128\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pprint import pprint as pp\n",
    "# loading the pickle file to a Python dict to display length and features\n",
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "print(\"Number of rows present in the data:\", len(data_dict))\n",
    "features = []\n",
    "poi_count = 0\n",
    "for k in data_dict.keys():\n",
    "    for d in data_dict[k]:\n",
    "        if d not in features:\n",
    "            features.append(d)\n",
    "    if data_dict[k]['poi']:\n",
    "        poi_count += 1\n",
    "print(\"Data features for any given row:\")\n",
    "pp(features)\n",
    "print(\"Number of features:\", len(features))\n",
    "print(\"Number of persons of interest in the set: \", poi_count)\n",
    "print(\"Non-PoIs: \", len(data_dict) - poi_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of those features, 'poi' contains Boolean True or False values representing whether a given individual was considered a \"person of interest\" (PoI) during the investigation into Enron's financial activities. By those values, of the 146 entries in the set 18 are PoIs, leaving 128 non-PoIs.\n",
    "\n",
    "Financial and email feature values are typically integers--excepting 'email_address'--representing either dollar amounts or counts for each. Given what little correlation to the other data is possible for email addresses, I decided to remove them as a feature, reducing the total count of features to 20.\n",
    "\n",
    "Among the remaining initial features, there were a lot of \"missing\" values for entries in the data, represented by 'NaN':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing email addresses... done.\n",
      "\n",
      "Remaining features in the dataset: 20\n",
      "\n",
      "'NaN' (missing) values in the dataset, by feature:\n",
      "{'bonus': 64,\n",
      " 'deferral_payments': 107,\n",
      " 'deferred_income': 97,\n",
      " 'director_fees': 129,\n",
      " 'exercised_stock_options': 44,\n",
      " 'expenses': 51,\n",
      " 'from_messages': 60,\n",
      " 'from_poi_to_this_person': 60,\n",
      " 'from_this_person_to_poi': 60,\n",
      " 'loan_advances': 142,\n",
      " 'long_term_incentive': 80,\n",
      " 'other': 53,\n",
      " 'poi': 0,\n",
      " 'restricted_stock': 36,\n",
      " 'restricted_stock_deferred': 128,\n",
      " 'salary': 51,\n",
      " 'shared_receipt_with_poi': 60,\n",
      " 'to_messages': 60,\n",
      " 'total_payments': 21,\n",
      " 'total_stock_value': 20}\n"
     ]
    }
   ],
   "source": [
    "print(\"Removing email addresses... \", end='')\n",
    "for k in data_dict.keys():\n",
    "  if 'email_address' in data_dict[k].keys():\n",
    "    data_dict[k].pop('email_address', 0)\n",
    "print(\"done.\")\n",
    "print(\"\\nRemaining features in the dataset:\", len(features)-1)\n",
    "missing_values = {'salary' : 0,\n",
    "                  'to_messages' : 0,\n",
    "                  'deferral_payments' : 0,\n",
    "                  'total_payments' : 0,\n",
    "                  'loan_advances' : 0,\n",
    "                  'bonus' : 0,\n",
    "                  'restricted_stock_deferred' : 0,\n",
    "                  'deferred_income' : 0,\n",
    "                  'total_stock_value' : 0,\n",
    "                  'expenses' : 0,\n",
    "                  'from_poi_to_this_person' : 0,\n",
    "                  'exercised_stock_options' : 0,\n",
    "                  'from_messages' : 0,\n",
    "                  'other' : 0,\n",
    "                  'from_this_person_to_poi' : 0,\n",
    "                  'poi' : 0,\n",
    "                  'long_term_incentive' : 0,\n",
    "                  'shared_receipt_with_poi' : 0,\n",
    "                  'restricted_stock' : 0,\n",
    "                  'director_fees' : 0}\n",
    "for k in data_dict.keys():\n",
    "    for d in data_dict[k]:\n",
    "        if data_dict[k][d] == 'NaN':\n",
    "            missing_values[d] += 1\n",
    "print(\"\\n'NaN' (missing) values in the dataset, by feature:\")\n",
    "pp(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 'poi' appears to have a value for every row. Missing values for financial data features vary, with most rows having varying, multiple 'NaN's across them, but since these can be interpreted as meaning '0' (given the reference provided in enron61702insiderpay.pdf, included with the project files), financial data for each entry can be interpreted as complete.\n",
    "\n",
    "For email metadata features, though, there's a common '60' missing values across all but 'email_addresses'. This is best understood by observing that across all rows, either an entry has integer values for all email features, or it has only 'NaN' values for each, meaning emails to or from those individuals were not present in the underlying data, so email metadata for each entry can be interpreted as complete.\n",
    "\n",
    "When looking through `tester.py`'s code in order to understand its internal treatment of the dataset, I noticed the default treatment for 'NaN' values, based on the `feature_format.featureFormat()` call in `tester.py`'s `test_classifier` method:\n",
    "```\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "```\n",
    "Given that call, `feature_format.featureFormat()` has a default treatment for 'NaN' values which is being used:\n",
    "```\n",
    "def featureFormat( dictionary, features, remove_NaN=True...\n",
    "  ...<lots of code ommitted>...\n",
    "  if value==\"NaN\" and remove_NaN:\n",
    "    value = 0\n",
    "```\n",
    "This means that 'NaN' values will be replaced with integer zeroes upon testing. As an alternative, missing values *could* be imputed, replaced with values based on other features in those rows, statistics from the distribution of that feature across all rows, or some combination of both. [This](https://towardsdatascience.com/why-using-a-mean-for-missing-data-is-a-bad-idea-alternative-imputation-algorithms-837c731c1008) article includes descriptions of methods, limitations, and drawbacks for imputation, and also cites [this](http://www.stat.columbia.edu/~gelman/arm/missing.pdf) paper which goes into greater detail. Besides the points raised in those references, given what I've come to understand about the features in this dataset, imputation may simply be inappropriate: if missing data in this set is \"best\" represented as zeroes, imputation would effectively result in \"false\" data, and therefore potentially inaccurate results from classification predictions based on that \"false\" data.\n",
    "\n",
    "Regardless, since rows with many 'NaN' values will result in zeroes on the distributions for related features, I decided to examine rows with more than 15 features containing 'NaN' values, since those would be rows which not only lacked email metadata (five features, 'to_messages', 'from_messages', 'from_poi_to_this_person', 'from_this_person_to_poi', and 'shared_receipt_with_poi') but would also have few nonmissing values for financial features, meaning relatively little contribution to predictive classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with less than 5 nonmissing values:\n",
      "CHRISTODOULOU DIOMEDES\n",
      "  total_stock_value : 6077885\n",
      "  exercised_stock_options : 5127155\n",
      "  poi : False\n",
      "  restricted_stock : 950730\n",
      "CLINE KENNETH W\n",
      "  restricted_stock_deferred : -472568\n",
      "  total_stock_value : 189518\n",
      "  poi : False\n",
      "  restricted_stock : 662086\n",
      "GILLIS JOHN\n",
      "  total_stock_value : 85641\n",
      "  exercised_stock_options : 9803\n",
      "  poi : False\n",
      "  restricted_stock : 75838\n",
      "GRAMM WENDY L\n",
      "  total_payments : 119292\n",
      "  poi : False\n",
      "  director_fees : 119292\n",
      "LOCKHART EUGENE E\n",
      "  poi : False\n",
      "SAVAGE FRANK\n",
      "  total_payments : 3750\n",
      "  deferred_income : -121284\n",
      "  poi : False\n",
      "  director_fees : 125034\n",
      "SCRIMSHAW MATTHEW\n",
      "  total_stock_value : 759557\n",
      "  exercised_stock_options : 759557\n",
      "  poi : False\n",
      "THE TRAVEL AGENCY IN THE PARK\n",
      "  total_payments : 362096\n",
      "  other : 362096\n",
      "  poi : False\n",
      "WAKEHAM JOHN\n",
      "  total_payments : 213071\n",
      "  expenses : 103773\n",
      "  poi : False\n",
      "  director_fees : 109298\n",
      "WHALEY DAVID A\n",
      "  total_stock_value : 98718\n",
      "  exercised_stock_options : 98718\n",
      "  poi : False\n",
      "WODRASKA JOHN\n",
      "  total_payments : 189583\n",
      "  other : 189583\n",
      "  poi : False\n",
      "WROBEL BRUCE\n",
      "  total_stock_value : 139130\n",
      "  exercised_stock_options : 139130\n",
      "  poi : False\n",
      "Number of entries with less than 5 nonmissing values:\n",
      "  12\n"
     ]
    }
   ],
   "source": [
    "# populating a dictionary entries for each entry in data_dict\n",
    "#   with each entry assigned the number of 'NaN'-valued features for the same\n",
    "empty_features = {}\n",
    "for k in data_dict.keys():\n",
    "  empty_features[k] = []\n",
    "  for d in data_dict[k]:\n",
    "    if data_dict[k][d] == 'NaN':\n",
    "      empty_features[k].append(1)\n",
    "  empty_features[k] = sum(empty_features[k])\n",
    "count = 0\n",
    "print(\"Entries with less than 5 nonmissing values:\")\n",
    "for k in sorted(empty_features):\n",
    "  if empty_features[k] > 15:\n",
    "    print(k)\n",
    "    for d in data_dict[k]:\n",
    "      if data_dict[k][d] != 'NaN':\n",
    "        print(\"  %s : %s\" % (d, data_dict[k][d]))\n",
    "    count += 1\n",
    "print(\"Number of entries with less than 5 nonmissing values:\")\n",
    "print(\" \",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What first struck me was that there were relatively few such rows, and that none of these rows were for persons of interest. Next, I noticed that 'LOCKHART EUGENE E' had only one nonmissing value, 'poi', meaning its values for all other features would be set to zero upon testing. Also, 'THE TRAVEL AGENCY IN THE PARK', appears to represent a business, rather than an individual. A description for this entry is footnoted in enron61702insiderpay.pdf:\n",
    ">Payments were made by Enron employees on account of business-related travel to The Travel Agency in the Park (later Alliance Worldwide), which was co-\n",
    "owned by the sister of Enron's former Chairman. Payments made by the Debtor to reimburse employees for these expenses have not been included.\n",
    "\n",
    "The other rows shown above may have relatively little data that would be non-zero upon testing, but given that they are all non-PoIs, the data they *do* hold may well contribute to classification.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "As previously mentioned, 'email_address' holds no correlatable values, so I've removed that feature entirely, popping it from every entry in the dataset.\n",
    "\n",
    "Besides that, inspection of the dataset resulted in these rows standing out:  \n",
    "  - 'TOTAL'\n",
    "  - 'THE TRAVEL AGENCY IN THE PARK'\n",
    "  - 'LOCKHART EUGENE E'\n",
    "\n",
    "Having already shown the contents of 'THE TRAVEL AGENCY IN THE PARK' and 'LOCKHART EUGENE E', here's 'TOTAL':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'TOTAL':\n",
      "{'bonus': 97343619,\n",
      " 'deferral_payments': 32083396,\n",
      " 'deferred_income': -27992891,\n",
      " 'director_fees': 1398517,\n",
      " 'exercised_stock_options': 311764000,\n",
      " 'expenses': 5235198,\n",
      " 'from_messages': 'NaN',\n",
      " 'from_poi_to_this_person': 'NaN',\n",
      " 'from_this_person_to_poi': 'NaN',\n",
      " 'loan_advances': 83925000,\n",
      " 'long_term_incentive': 48521928,\n",
      " 'other': 42667589,\n",
      " 'poi': False,\n",
      " 'restricted_stock': 130322299,\n",
      " 'restricted_stock_deferred': -7576788,\n",
      " 'salary': 26704229,\n",
      " 'shared_receipt_with_poi': 'NaN',\n",
      " 'to_messages': 'NaN',\n",
      " 'total_payments': 309886585,\n",
      " 'total_stock_value': 434509511}\n"
     ]
    }
   ],
   "source": [
    "print(\"'TOTAL':\")\n",
    "pp(data_dict['TOTAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an aggregate row, and an incomplete one in that it applies only to financial data--it's clearly a carryover from the financial data represented in enron61702insiderpay.pdf. Being an aggregate, it could only be considered a non-person, and its upper outlier status for all non-missing values would only introduce inaccuracy to a classification model, and so should be removed.\n",
    "\n",
    "'THE TRAVEL AGENCY IN THE PARK' is cited (see above) as \"owned by the sister of Enron's former Chairman\", but her name is not immediately apparent, and after spending some time in search of her name I was not able to confirm whether or not she was included in this dataset. Regardless, like 'TOTAL', this entry does not represent an individual and should be removed.\n",
    "\n",
    "The entry for 'LOCKHART EUGENE E' is composed of only 'NaN' values and `'poi' : False`, so its data for all features used in predictive classification will be filled with zeroes. Since my intuition is that an individual with no financial or email interaction with Enron is *appropriately* labeled a non-PoI, and since there are relatively few entries in the dataset as a whole, I've opted to leave this entry in place.\n",
    "\n",
    "Given the above rationale, I removed 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows remaining in the dataset: 144\n"
     ]
    }
   ],
   "source": [
    "# removing aggregate row\n",
    "data_dict.pop('TOTAL', 0)\n",
    "# removing non-person row\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK', 0)\n",
    "print(\"Rows remaining in the dataset:\",len(data_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In thinking about checking the data for any other issues that required cleaning, I figured out pretty quickly that it would be difficult to check the dataset's email metadata, as it's been compiled via \"to\" and \"from\" fields across all ~500,000 emails. I *can* check financial data in the set with basic calculations, though, which I did by way of adding together financial payment features and checking them against 'total_payments':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data for issues related to 'total_payments'.........found!\n",
      "  Rows with issues related to 'total_payments' found:\n",
      "{'BELFER ROBERT': {'bonus': 'NaN',\n",
      "                   'deferral_payments': -102500,\n",
      "                   'deferred_income': 'NaN',\n",
      "                   'director_fees': 3285,\n",
      "                   'exercised_stock_options': 3285,\n",
      "                   'expenses': 'NaN',\n",
      "                   'from_messages': 'NaN',\n",
      "                   'from_poi_to_this_person': 'NaN',\n",
      "                   'from_this_person_to_poi': 'NaN',\n",
      "                   'loan_advances': 'NaN',\n",
      "                   'long_term_incentive': 'NaN',\n",
      "                   'other': 'NaN',\n",
      "                   'poi': False,\n",
      "                   'restricted_stock': 'NaN',\n",
      "                   'restricted_stock_deferred': 44093,\n",
      "                   'salary': 'NaN',\n",
      "                   'shared_receipt_with_poi': 'NaN',\n",
      "                   'to_messages': 'NaN',\n",
      "                   'total_payments': 102500,\n",
      "                   'total_stock_value': -44093},\n",
      " 'BHATNAGAR SANJAY': {'bonus': 'NaN',\n",
      "                      'deferral_payments': 'NaN',\n",
      "                      'deferred_income': 'NaN',\n",
      "                      'director_fees': 137864,\n",
      "                      'exercised_stock_options': 2604490,\n",
      "                      'expenses': 'NaN',\n",
      "                      'from_messages': 29,\n",
      "                      'from_poi_to_this_person': 0,\n",
      "                      'from_this_person_to_poi': 1,\n",
      "                      'loan_advances': 'NaN',\n",
      "                      'long_term_incentive': 'NaN',\n",
      "                      'other': 137864,\n",
      "                      'poi': False,\n",
      "                      'restricted_stock': -2604490,\n",
      "                      'restricted_stock_deferred': 15456290,\n",
      "                      'salary': 'NaN',\n",
      "                      'shared_receipt_with_poi': 463,\n",
      "                      'to_messages': 523,\n",
      "                      'total_payments': 15456290,\n",
      "                      'total_stock_value': 'NaN'}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking data for issues related to 'total_payments'.........\", end='')\n",
    "payment_financial_features = ['salary',\n",
    "                              'bonus',\n",
    "                              'long_term_incentive',\n",
    "                              'expenses',\n",
    "                              'director_fees',\n",
    "                              'other',\n",
    "                              'loan_advances',\n",
    "                              'deferred_income',\n",
    "                              'deferral_payments']\n",
    "problem_entries = {}\n",
    "# Iterate over each row, check sum of above features against total_payments,\n",
    "#   adding rows with mismatch to problem_entries\n",
    "for k in data_dict.keys():\n",
    "  total_payments_check = 0\n",
    "  for d in data_dict[k]:\n",
    "    if d in payment_financial_features and data_dict[k][d] != 'NaN':\n",
    "      total_payments_check += data_dict[k][d]\n",
    "  if data_dict[k]['total_payments'] != 'NaN' and \\\n",
    "                        total_payments_check != data_dict[k]['total_payments']:\n",
    "    problem_entries[k] = data_dict[k]\n",
    "from pprint import pprint as pp\n",
    "if len(problem_entries):\n",
    "  print(\"found!\")\n",
    "  print(\"  Rows with issues related to 'total_payments' found:\")\n",
    "  pp(problem_entries)\n",
    "else:\n",
    "  print(\"none.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I checked these rows' data against their entries in enron61702insiderpay.pdf, and it was readily apparent that their financial data values were \"shifted\", such that Belfer's were shifted to the right, and Bhatnagar's to the left. Given the organization of these entries' features in the dictionary, the simplest way to correct these problems was to create new entries for each with the correct financial feature values from the pdf. I left the email metadata features' values \"as-found\", since verifying those would require an inordinate amount of effort, and there wasn't any apparent reason to distrust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'BELFER ROBERT', lines marked with '#' were affected by an apparent shift\n",
    "#   in values, corrected here with values from reference.\n",
    "# Email data left as-found.\n",
    "belfer_corrected = {'bonus': 'NaN',\n",
    "                    'deferral_payments': 0,                  #\n",
    "                    'deferred_income': -102500,              #\n",
    "                    'director_fees': 102500,                 #\n",
    "                    'exercised_stock_options': 0,            #\n",
    "                    'expenses': 3285,                        #\n",
    "                    'from_messages': 'NaN',\n",
    "                    'from_poi_to_this_person': 'NaN',\n",
    "                    'from_this_person_to_poi': 'NaN',\n",
    "                    'loan_advances': 'NaN',\n",
    "                    'long_term_incentive': 'NaN',\n",
    "                    'other': 'NaN',\n",
    "                    'poi': False,\n",
    "                    'restricted_stock': 44093,                #\n",
    "                    'restricted_stock_deferred': -44093,      #\n",
    "                    'salary': 'NaN',\n",
    "                    'shared_receipt_with_poi': 'NaN',\n",
    "                    'to_messages': 'NaN',\n",
    "                    'total_payments': 3285,                   #\n",
    "                    'total_stock_value': 0}                   #\n",
    "\n",
    "# Likewise, for 'BHATNAGAR SANJAY', lines marked with '#' were affected by an\n",
    "#   apparent shift in data, corrected here with values from reference.\n",
    "# Email data left as-found.\n",
    "bhatnagar_corrected = {'bonus': 'NaN',\n",
    "                       'deferral_payments': 'NaN',\n",
    "                       'deferred_income': 'NaN',\n",
    "                       'director_fees': 0,                    #\n",
    "                       'exercised_stock_options': 15456290,   #\n",
    "                       'expenses': 137864,                    #\n",
    "                       'from_messages': 29,\n",
    "                       'from_poi_to_this_person': 0,\n",
    "                       'from_this_person_to_poi': 1,\n",
    "                       'loan_advances': 'NaN',\n",
    "                       'long_term_incentive': 'NaN',\n",
    "                       'other': 0,                            #\n",
    "                       'poi': False,\n",
    "                       'restricted_stock': 2604490,           #\n",
    "                       'restricted_stock_deferred': -2604490, #\n",
    "                       'salary': 'NaN',\n",
    "                       'shared_receipt_with_poi': 463,\n",
    "                       'to_messages': 523,\n",
    "                       'total_payments': 137864,              #\n",
    "                       'total_stock_value': 15456290}         #\n",
    "\n",
    "data_dict['BELFER ROBERT'] = belfer_corrected\n",
    "data_dict['BHATNAGAR SANJAY'] = bhatnagar_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having overwritten the faulty entries, I checked my payment financial data totals again to verify the fix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-checking data for issues related to 'total_payments'.....none.\n"
     ]
    }
   ],
   "source": [
    "# Repeating check to verify changes\n",
    "print(\"Re-checking data for issues related to 'total_payments'.....\", end='')\n",
    "problem_entries = {}\n",
    "for k in data_dict.keys():\n",
    "  total_payments_check = 0\n",
    "  for d in data_dict[k]:\n",
    "    if d in payment_financial_features and data_dict[k][d] != 'NaN':\n",
    "      total_payments_check += data_dict[k][d]\n",
    "  if data_dict[k]['total_payments'] != 'NaN' and \\\n",
    "    total_payments_check != data_dict[k]['total_payments']:\n",
    "    problem_entries[k] = data_dict[k]\n",
    "\n",
    "if len(problem_entries):\n",
    "  print(\"found!\")\n",
    "  print(\"  Rows with issues related to 'total_payments' found:\")\n",
    "  pp(problem_entries)\n",
    "else:\n",
    "  print(\"none.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers and Data Cleaning Decisions\n",
    "\n",
    "It was a little tricky thinking about what might be considered outliers among the data in this set. Given the nature of what made any given individual a PoI or not, extremely high or low values among the entries could be considered appropriate to their true or false 'poi' status. Any \"conflicting\" combinations of values among an entry's features could potentially be indicative of feature importances, too. Compounding both those prospects, the dataset is relatively sparse in both its number of rows and nonzero values for features. After considering the possible impact of further reduction of the dataset, it seemed prudent to trim and clean as little as possible: I opted to only remove feature 'email_address', and rows 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK'.\n",
    "\n",
    "#### From Question 2\n",
    ">... As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) ...\n",
    "\n",
    "### Feature Creation\n",
    "\n",
    "Since my exploration of the data revealed that 'NaN' values for financial features varied widely from row to row, I realized that I could not rely on any specific set of financial features being present for creation of a calculated feature. The only reliably-present set of features was the five email metadata features--as previously mentioned, rows either have integer values for all of them, or else 'NaN' for all of them--So, the features I introduced to the dataset were all fractions based on email metadata:\n",
    " - ratio of emails sent to PoIs to emails sent generally:  \n",
    "   `to_poi_from_messages_ratio = from_this_person_to_poi / from_message`\n",
    " - ratio of emails received from PoIs to emails received generally:  \n",
    "   `from_poi_to_messages_ratio = from_poi_to_this_person / to_messages`\n",
    " - ratio of emails having shared recipt with PoI to emails received generally:  \n",
    "   `shared_receipt_to_messages_ratio = shared_receipt_with_poi / to_messages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features... done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating features... \", end='')\n",
    "for k in data_dict.keys():\n",
    "  from_messages = True if \\\n",
    "    (data_dict[k]['from_messages'] != 'NaN') else False\n",
    "  to_messages = True if \\\n",
    "    (data_dict[k]['to_messages'] != 'NaN') else False\n",
    "  to_poi = True if \\\n",
    "    (data_dict[k]['from_this_person_to_poi'] != 'NaN') else  False\n",
    "  from_poi = True if \\\n",
    "    (data_dict[k]['from_poi_to_this_person'] != 'NaN') else False\n",
    "  shared_receipt = True if \\\n",
    "    (data_dict[k]['shared_receipt_with_poi'] != 'NaN') else False\n",
    "\n",
    "  # ratio of emails sent to PoIs to emails sent generally:\n",
    "  # to_poi_from_messages_ratio = from_this_person_to_poi / from_messages\n",
    "  if to_poi and from_messages:\n",
    "    data_dict[k]['to_poi_from_messages_ratio'] = \\\n",
    "       data_dict[k]['from_this_person_to_poi'] / data_dict[k]['from_messages']\n",
    "  else:\n",
    "    data_dict[k]['to_poi_from_messages_ratio'] = 'NaN'\n",
    "\n",
    "  # ratio of emails received from PoIs to emails received generally:\n",
    "  # from_poi_to_messages_ratio = from_poi_to_this_person / to_messages\n",
    "  if from_poi and to_messages:\n",
    "    data_dict[k]['from_poi_to_messages_ratio'] = \\\n",
    "          data_dict[k]['from_poi_to_this_person'] / data_dict[k]['to_messages']\n",
    "  else:\n",
    "    data_dict[k]['from_poi_to_messages_ratio'] = 'NaN'\n",
    "  \n",
    "  # ratio of emails having shared recipt with PoIs to emails received generally:\n",
    "  # shared_receipt_to_messages_ratio = shared_receipt_with_poi / to_messages\n",
    "  if shared_receipt and to_messages:\n",
    "    data_dict[k]['shared_receipt_to_messages_ratio'] = \\\n",
    "       data_dict[k]['shared_receipt_with_poi'] / data_dict[k]['to_messages']\n",
    "  else:\n",
    "    data_dict[k]['shared_receipt_to_messages_ratio'] = 'NaN'\n",
    "print(\"done.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I checked these created features via this code, observing the values created for each entry (results omitted due to length):\n",
    "```\n",
    "for k in data_dict.keys():\n",
    "  print(k)\n",
    "  print(\" to\", data_dict[k]['to_messages'])\n",
    "  print(\" from\", data_dict[k]['from_messages'])\n",
    "  print(\" to_poi\", data_dict[k]['from_this_person_to_poi'])\n",
    "  print(\" to poi/from\",data_dict[k]['to_poi_from_messages_ratio'])\n",
    "  print(\" from_poi\", data_dict[k]['from_poi_to_this_person'])\n",
    "  print(\" from poi/to\",data_dict[k]['from_poi_to_messages_ratio'])\n",
    "  print(\" shared\",data_dict[k]['shared_receipt_with_poi'])\n",
    "  print(\" shared/to \",data_dict[k]['shared_receipt_to_messages_ratio'])\n",
    "```\n",
    "Excepting a value of 1.0011 for 'GLISAN JR BEN F', caused by its 'shared_receipt_with_poi' value exceeding its 'to_messages' value by 1, all values for these created features were either decimal values between 0 and 1, or 'NaN', as intended.\n",
    "With the removal of 'email_address' and inclusion of 'to_poi_from_messages_ratio', 'from_poi_to_messages_ratio', and 'shared_receipt_to_messages_ratio', non-target features in the dataset number 22, with 'poi' remaining as a target feature.\n",
    "\n",
    "#### From Question 3\n",
    ">...What [algorithms] did you try? How did model performance differ between algorithms?\n",
    "\n",
    "### Algorithms Tried\n",
    "\n",
    "I evaluated performance differences between DecisionTreeClassifier, KNeighborsClassifier (K Nearest Neighbors), and GaussianNB (Gaussian Naive Bayes). Since I'd reviewed `tester.py` and seen that its `test_classification()` method applied K-fold cross-validation prior to iterative fitting and prediction, I decided to mimic that function's approach \"whole-cloth\" in order to base my algorithm choice on the same metrics used in grading: accuracy, precision, recall, F1, and F2, each as ratios calculated from sums for comparisons of predictions and labels across 1000 testing-training splits of the dataset, equivalent to a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying several classifiers with default settings for comparison...\n",
      "\n",
      "KNeighborsClassifier()\n",
      "  Predictions: 15000\n",
      "  Accuracy: 0.87513\n",
      "  Precision: 0.61609  Recall: 0.16850\n",
      "  F1: 0.26463  F2: 0.19715 \n",
      "\n",
      "DecisionTreeClassifier()\n",
      "  Predictions: 15000\n",
      "  Accuracy: 0.80460\n",
      "  Precision: 0.26383  Recall: 0.26000\n",
      "  F1: 0.26190  F2: 0.26076 \n",
      "\n",
      "GaussianNB()\n",
      "  Predictions: 15000\n",
      "  Accuracy: 0.76353\n",
      "  Precision: 0.24564  Recall: 0.37350\n",
      "  F1: 0.29637  F2: 0.33828 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors         import KNeighborsClassifier\n",
    "from sklearn.tree              import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes       import GaussianNB\n",
    "from sklearn.ensemble          import AdaBoostClassifier\n",
    "from sklearn.model_selection   import StratifiedShuffleSplit\n",
    "\n",
    "# 4-A Function definition for classifier testing, validation, evaluation\n",
    "def classifier_test(clf, dataset, feature_list, folds = 1000):\n",
    "  '''\n",
    "  Based on code used in tester.py, with equivalent functionality, this function\n",
    "evaluates classifier performance through cross-validation via StratifiedShuffleSplit(),\n",
    "default 1000 splits for training and testing sets.\n",
    "  Written primarily for personal comprehension of the testing method used in grading\n",
    "results, and to apply the same metrics used in grading to validation and evaluation of\n",
    "classifiers.\n",
    "\n",
    "parameters:\n",
    "\n",
    "clf:\n",
    "  sklearn classifier, must support *.fit, *.predict.\n",
    "  \n",
    "dataset:\n",
    "  object compatible with Python dict, must have key entries containing features and\n",
    "values compatible with feature_list.\n",
    "\n",
    "feature_list:\n",
    "  Python list, must contain strings matching features present in dict passed to\n",
    "'dataset'.\n",
    "  \n",
    "folds:\n",
    "  integer, default 1000, controls splits applied for cross validation via\n",
    "StratifiedShuffleSplit().\n",
    "\n",
    "output:\n",
    "  Displays predictions made and performance results:\n",
    "    Accuracy, Precision, Recall, F1, F2.\n",
    "  '''\n",
    "  data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "  labels, features = targetFeatureSplit(data)\n",
    "  cv = StratifiedShuffleSplit(n_splits=folds, random_state = 42)\n",
    "  true_neg  = 0\n",
    "  false_neg = 0\n",
    "  true_pos  = 0\n",
    "  false_pos = 0\n",
    "  for train_idx, test_idx in cv.split(features, labels):\n",
    "    features_train = []\n",
    "    labels_train   = []\n",
    "    features_test  = []\n",
    "    labels_test    = []\n",
    "    for ii in train_idx:\n",
    "      features_train.append(features[ii])\n",
    "      labels_train.append(labels[ii])\n",
    "    for jj in test_idx:\n",
    "      features_test.append(features[jj])\n",
    "      labels_test.append(labels[jj])\n",
    "\n",
    "    # fit the classifier using training set, and test on test set\n",
    "    clf.fit(features_train, labels_train)\n",
    "    predictions = clf.predict(features_test)\n",
    "    for prediction, truth in zip(predictions, labels_test):\n",
    "      if prediction == 0 and truth == 0:\n",
    "        true_neg += 1\n",
    "      elif prediction == 0 and truth == 1:\n",
    "        false_neg += 1\n",
    "      elif prediction == 1 and truth == 0:\n",
    "        false_pos += 1\n",
    "      elif prediction == 1 and truth == 1:\n",
    "        true_pos += 1\n",
    "      else:\n",
    "        print(\"Warning: Found a predicted label not == 0 or 1.\")\n",
    "        print(\"All predictions should take value 0 or 1.\")\n",
    "        print(\"Evaluating performance for processed predictions:\")\n",
    "        break\n",
    "  try:\n",
    "    total_pred = true_neg + false_neg + false_pos + true_pos\n",
    "    accuracy = 1.0 * (true_pos + true_neg) / total_pred\n",
    "    precision = 1.0 * true_pos / (true_pos + false_pos)\n",
    "    recall = 1.0 * true_pos / (true_pos + false_neg)\n",
    "    f1 = 2.0 * true_pos / (2 * true_pos + false_pos + false_neg)\n",
    "    f2 = (1 + 2.0 * 2.0) * precision * recall / (4 * precision + recall)\n",
    "    print(clf)\n",
    "    print(\"  Predictions: %d\" % total_pred)\n",
    "    print(\"  Accuracy: %.5f\\n  Precision: %.5f  Recall: %.5f\" % \\\n",
    "          (accuracy, precision, recall))\n",
    "    print(\"  F1: %.5f  F2: %.5f\" % (f1, f2), \"\\n\")\n",
    "  except:\n",
    "    print(\"Performance calculations failed.\")\n",
    "    print(\"Precision or recall may be undefined (no true positives).\")\n",
    "\n",
    "# 4-B  Iteration over a list of classifiers\n",
    "# (see references.txt for code example source)\n",
    "classifiers = [KNeighborsClassifier(),\n",
    "               DecisionTreeClassifier(),\n",
    "               GaussianNB()]\n",
    "\n",
    "print(\"Trying several classifiers with default settings for comparison...\\n\")\n",
    "for classifier in classifiers:\n",
    "  classifier_test(classifier, data_dict, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With default settings and  features used, the above results were yielded for KNeighborsClassifier, DecisionTreeClassifier, and GaussianNB. KNeighBorsClassifier ranked highest in accuracy and precision, but lowest in recall. DecisionTreeClassifier slightly outperformed GaussianNB in accuracy and precision, but was lesser in recall. GaussianNB had the greatest recall, but least accuracy or precision.\n",
    "\n",
    "Given those results, I decided to perform more extensive testing and parameter tuning for KNeighborsClassifier and DecisionTreeClassifier, and ultimately discarded KNeighborsClassifier when tuning for feature selection and parameter settings could not result in a sufficient recall score. Relative inflexibility for GaussianNB would not permit much tuning for its parameters, so I ruled it out early on, despite its relatively high recall score.\n",
    "\n",
    "#### From Question 2\n",
    "> What features did you end up using in your POI identifier, and what selection process did you use to pick them?...\n",
    "\n",
    "### Feature Importance Inspection\n",
    "\n",
    "Having tested a few classifiers, I examined feature importances by a number of different methods (DecisionTreeClassifier's built-in 'Gini' impurity metric, f_classif's 'ANOVA' f-values and p-values, various settings for both via GridSearchCV) before finding meaningful increases in my algorithm's performance by using 'mutual information' values derived from `sklearn.feature_selection.mutual_info_classif()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features and labels... done.\n",
      "\n",
      "\n",
      "Feature importance by mutual_info_classif:\n",
      " ('mutual info' with regard to 'poi' target)\n",
      "  1 - 'expenses' \n",
      "        0.08268\n",
      "  2 - 'bonus' \n",
      "        0.07840\n",
      "  3 - 'other' \n",
      "        0.06814\n",
      "  4 - 'shared_receipt_with_poi' \n",
      "        0.06151\n",
      "  5 - 'to_poi_from_messages_ratio' \n",
      "        0.05606\n",
      "  6 - 'total_stock_value' \n",
      "        0.03748\n",
      "  7 - 'restricted_stock' \n",
      "        0.03558\n",
      "  8 - 'from_poi_to_messages_ratio' \n",
      "        0.02978\n",
      "  9 - 'shared_receipt_to_messages_ratio' \n",
      "        0.02889\n",
      "  10 - 'from_this_person_to_poi' \n",
      "        0.02545\n",
      "  11 - 'director_fees' \n",
      "        0.02352\n",
      "  12 - 'salary' \n",
      "        0.02239\n",
      "  13 - 'total_payments' \n",
      "        0.01453\n",
      "  14 - 'from_poi_to_this_person' \n",
      "        0.01436\n",
      "  15 - 'from_messages' \n",
      "        0.01259\n",
      "  16 - 'exercised_stock_options' \n",
      "        0.01177\n",
      "  17 - 'to_messages' \n",
      "        0.00828\n",
      "  18 - 'long_term_incentive' \n",
      "        0.00677\n",
      "  19 - 'restricted_stock_deferred' \n",
      "        0.00313\n",
      "  20 - 'loan_advances' \n",
      "        0.00162\n",
      "  21 - 'deferred_income' \n",
      "        0.00071\n",
      "  22 - 'deferral_payments' \n",
      "        0.00000\n"
     ]
    }
   ],
   "source": [
    "# for feature, label extraction\n",
    "features_list = ['poi',\n",
    "                 'salary',\n",
    "                 'bonus',\n",
    "                 'long_term_incentive',\n",
    "                 'expenses',\n",
    "                 'director_fees',\n",
    "                 'other',\n",
    "                 'loan_advances',\n",
    "                 'deferred_income',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'exercised_stock_options',\n",
    "                 'restricted_stock',\n",
    "                 'total_stock_value',\n",
    "                 'from_messages',\n",
    "                 'to_messages',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'from_poi_to_messages_ratio',\n",
    "                 'to_poi_from_messages_ratio',\n",
    "                 'shared_receipt_to_messages_ratio']\n",
    "\n",
    "# Extracting features and labels from dataset for local testing\n",
    "print(\"Extracting features and labels... \", end='')\n",
    "data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "print(\"done.\\n\")\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "print(\"\\nFeature importance by mutual_info_classif:\")\n",
    "print(\" ('mutual info' with regard to 'poi' target)\")\n",
    "# sorting feature names by magnitude of mutual information with 'poi'\n",
    "# (see references.txt for code example used with zip() sorting of two lists)\n",
    "mutual_info = sorted(zip(list(mutual_info_classif(features, labels)),\n",
    "                         features_list[1:]), reverse = True)\n",
    "for i in range(len(mutual_info)):\n",
    "  print(\" \", i+1, \"- '%s'\" % mutual_info[i][1],\n",
    "        \"\\n        %.5f\"   % mutual_info[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the exact meaning of these scores is slightly obfuscated by the density of the subject matter involved, this quote from the [wiki entry for mutual information](https://en.wikipedia.org/wiki/Mutual_information) is relatively concise:\n",
    "\n",
    ">In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the \"amount of information\" (in units such as shannons, commonly called bits) obtained about one random variable through observing the other random variable. The concept of mutual information is intimately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected \"amount of information\" held in a random variable.\n",
    "\n",
    ">Not limited to real-valued random variables and linear dependence like the correlation coefficient, MI is more general and determines how different the joint distribution of the pair ( X , Y ) is to the product of the marginal distributions of X and Y. MI is the expected value of the pointwise mutual information (PMI).\n",
    "\n",
    "Given multiple executions of the above script, slightly varying results will be returned due to function behavior described in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html?highlight=mutual_info_classif#sklearn.feature_selection.mutual_info_classif)for `mutual_info_classif`:\n",
    "\n",
    ">The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n",
    "\n",
    "Because of that slight variation, I observed the results of multiple executions of a GridSearchCV (described later) involving this metric for feature selection, and was able to recognize the consistently high ranking of a particular subset of features.\n",
    "\n",
    "### Features Used in Final Algorithm\n",
    "\n",
    "My final algorithm wound up making use of only these features:\n",
    "\n",
    " - 'expenses',\n",
    " - 'other',\n",
    " - 'bonus',\n",
    " - 'to_poi_from_messages_ratio',\n",
    " - 'shared_receipt_with_poi'\n",
    "\n",
    "Making use of only these features in my algorithm testing (by the same method and function used above, described below) resulted in a roughly 0.2 increase in precision and recall, varying by 0.01~0.02 with the very slightly randomized performance inherent to Decision Tree classification.\n",
    "\n",
    "#### From Question 2\n",
    "\n",
    ">Did you have to do any scaling? Why or why not?\n",
    "\n",
    "I did not require scaling for my features, due to DecisionTreeClassifier not requiring such for successful performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Question 4\n",
    "> What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well? How did you tune the parameters of your particular algorithm? What parameters did you tune?...\n",
    "\n",
    "The basic meaning of 'tuning' parameters for a classification algorithm is iteratively changing settings for internal behavior of classifiers with the objective of increasing performance by way of minimizing process runtime and/or maximizing resulting evaluation metrics like accuracy, precision, recall, and/or composites like f1, f2. Not doing this 'well' might mean drastically increased time for optimization of results, and/or poorly optimized results which do not properly make use of the potential of a classifier. Parameter tuning is crucially important to maximizing that potential.\n",
    "\n",
    "I tuned my algorithm iteratively. First, by use of GridSearchCV with SelectKBest and DecisionTreeClassifier, optimizing parameters for maximized 'F1', which tended to result in maximized precision and accuracy.\n",
    "\n",
    "I attempted various permutations of feature selection metrics and splitting criterions, eventually finding that the best performance could be achieved by using mutual information as the feature selection metric (`SelectKBest(score_func=mutual_info_classif)`) and information gain as the splitting criterion (`DecisionTreeClassifier(criterion='entropy')`). Given the complexity involved in executing GridSearchCV, I minimized runtime by keeping those parameters fixed and then optimizing for 'k' in SelectKBest (number of features selected) and 'min_samples_split' in DecisionTreeClassifier (minimum samples required for internal node splitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "# Using mutual information as feature selection metric\n",
    "selector = SelectKBest(mutual_info_classif)\n",
    "# Using information gain as splitting criterion\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy')\n",
    "\n",
    "tune_pipe = Pipeline(steps=[('skb', selector),\n",
    "                            ('clf', classifier)])\n",
    "\n",
    "# Optimizing number of features and minimum number of samples for splitting\n",
    "grid_params = {'skb__k' : (3, 4, 5, 6, 7, 8, 9),\n",
    "                'clf__min_samples_split' : (3, 4, 5, 6, 7, 8, 9)}\n",
    "\n",
    "print(\"Trying GridSearchCV with\")\n",
    "pp(tune_pipe)\n",
    "print(\"over parameters:\")\n",
    "pp(grid_params)\n",
    "\n",
    "# Optimizing for maximized F1 in order to maximize precison and recall\n",
    "grid = GridSearchCV(tune_pipe, grid_params, scoring = 'f1', cv = 10,\n",
    "                    n_jobs = -1)\n",
    "grid.fit(features, labels)\n",
    "\n",
    "print(\"\\nResulting 'best' parameters for maximizing 'f1':\")\n",
    "pp(grid.best_params_)\n",
    "\n",
    "# sorting features by paired information gain scores\n",
    "grid_ftrs = sorted(zip(list(grid.best_estimator_.named_steps['skb'].scores_),\n",
    "                             features_list[1:]), reverse = True)\n",
    "# creating list to pass to k-fold testing method\n",
    "best_features = ['poi']\n",
    "print(\"\\nFeatures used:\")\n",
    "for i in range(grid.best_params_['skb__k']):\n",
    "  best_features.append(grid_ftrs[i][1])\n",
    "  print(\" \", i+1, \"- '%s'\" % grid_ftrs[i][1],\n",
    "        \"\\n        %.5f\"   % grid_ftrs[i][0])\n",
    "print('')\n",
    "\n",
    "# 5-B - Testing tuned parameters with 1000-fold cross validation\n",
    "classifier_test(grid.best_estimator_.named_steps['clf'],data_dict,\n",
    "                best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, I passed the \"best-parametered\" classifier and \"best-performing\" features from GridSearchCV to my previously-used 1000-fold cross validation testing function in order to observe performance results.\n",
    "\n",
    "With multiple executions of this code, resulting precision and recall were always good enough for project requirements, but tended to vary due to GridSearchCV resulting in slightly varying 'k', 'min_samples_split', and the exact features selected. This is likely normal, given the variation possible for mutual information scores (described above in the 'feature selection' section), and the variation possible for DecisionTreeClassifier by its node splitting method.\n",
    "\n",
    "By dint of observation, performance appeared to be greatest when 'min_samples_split' was 5, 'k' was 5, and the features used were as described above. Because of that, I changed my focus to manually testing those values for their repeated performance results from the testing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying DecisionTreeClassifier with parameter settings and feature\n",
      "  selection based on 'best' of varying results from optimization...\n",
      "  (features *reliably* top-ranked by 'mutual information' with 'poi')\n",
      "Features used:\n",
      "['expenses',\n",
      " 'bonus',\n",
      " 'other',\n",
      " 'to_poi_from_messages_ratio',\n",
      " 'shared_receipt_with_poi']\n",
      "DecisionTreeClassifier(criterion='entropy', min_samples_split=5)\n",
      "  Predictions: 13000\n",
      "  Accuracy: 0.84300\n",
      "  Precision: 0.48905  Recall: 0.45800\n",
      "  F1: 0.47302  F2: 0.46389 \n",
      "\n",
      "Testing final classifier via tester.py...\n",
      "DecisionTreeClassifier(criterion='entropy', min_samples_split=5)\n",
      "Accuracy: 0.84492  Precision: 0.49570  Recall: 0.46150\n",
      "    F1: 0.47799  F2: 0.46796\n",
      "Total predictions: 13000\n",
      "  True positives:  923  False positives:  939\n",
      "  False negatives: 1077  True negatives: 10061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# features (apart from 'poi') tending to be top-ranked in mutual information\n",
    "manual_features = ['poi',\n",
    "                   'expenses',\n",
    "                   'bonus',\n",
    "                   'other',\n",
    "                   'to_poi_from_messages_ratio',\n",
    "                   'shared_receipt_with_poi']\n",
    "\n",
    "# parameter settings tending to result in highest precision and recall\n",
    "clf = DecisionTreeClassifier(criterion = 'entropy',\n",
    "                             min_samples_split = 5)\n",
    "\n",
    "print(\"Trying DecisionTreeClassifier with parameter settings and feature\")\n",
    "print(\"  selection based on 'best' of varying results from optimization...\")\n",
    "print(\"  (features *reliably* top-ranked by 'mutual information' with 'poi')\")\n",
    "print(\"Features used:\")\n",
    "pp(manual_features[1:])\n",
    "classifier_test(clf, data_dict, manual_features)\n",
    "\n",
    "# Dumping classifier, dataset, features list, and running tester.py for final test\n",
    "import tester\n",
    "print(\"Testing final classifier via tester.py...\")\n",
    "tester.dump_classifier_and_data(clf, data_dict, manual_features)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Question 3\n",
    "> What algorithm did you end up using?\n",
    "\n",
    "As shown above, my final algorithm was a DecisionTreeClassifier set to use entropy (information gain) for its splitting criterion and a minimum of 5 samples for splitting of internal nodes, applied to a subset of features based on average top-ranks in mutual information with 'poi': 'expenses', 'bonus', 'other', 'to_poi_from_messages_ratio', and 'shared_receipt_with_poi'.\n",
    "\n",
    "#### Question 5\n",
    "> What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?\n",
    "\n",
    "Generally, validation is the verification of results from a predictive method by maintaining independence between its input in training and input in testing. If data from training and testing sets are mixed, a form of \"overfitting\" may occur, in which accuracy and other metrics may be higher than those possible for any independent data due to recognition of feature values from those used in training the classifier.\n",
    "\n",
    "I validated my analysis by using k-fold cross validation within the function shown above, `classifier_test`, in a manner equivalent to that applied by `tester.py`: the dataset was split by `StratifiedShuffleSplit` with a default of `1000` for its `n_splits` parameter, and performance metrics were calculated according to totals for all predictions, true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "#### Question 6\n",
    "> Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.\n",
    "\n",
    "With multiple executions of the testing shown above, accuracy tended to hover around 0.84, precision around 0.49, and recall around 0.46. This means that my algorithm was able to correctly predict a row's person-of-interest status around 84% of the time, correct positive predictions were only slightly less in number than incorrect positive predictions, and correct positive predictions were about 46% of all potentially positive predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
